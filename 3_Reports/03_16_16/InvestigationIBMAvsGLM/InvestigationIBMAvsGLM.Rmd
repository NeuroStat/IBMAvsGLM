---
title: "Investigating IBMA vs GLM"
author: "Han Bossier"
date: "6 april 2016"
output: 
    html_document:
      css: custom.css
      toc: no
      toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

## Introduction

In this report, I will zoom in on the differences between performing an image based meta-analysis using the classical tools (converting t-maps to effect sizes and calculate a weighted average based on fixed effects meta-analysis) and dealing with studies on a third level of the typical GLM procedure in an fMRI setting. Hence we get a classical meta-analysis (MA) approach versus a 3 level-GLM (GLM) approach. For more details about both procedures, I refer to the document IBMAvsMA. \
The things we will be considering are:

  1. Comparing maps on the second level GLM.
  2. Comparing maps on the final level (MA vs GLM).
  3. Using COPE and VARCOPE maps from the second level as input in a classical meta-analysis. 
  
> For quick reference, we have **three levels** in this report. The first one are the *subject* specific time series, the second level are *study* specific group analyses and the third level is the *meta-analysis*.

#### Data
In order to obtain results, we simulate fMRI *null* data on a grid of 4 x 4 x 4 voxels. The fMRI analysis consists of one blocked design condition (20 sec ON/OFF) with a TR of 2 sec and a total of 200 scans. Pure white noise only, no between study variability. We have 100 subjects in 5 studies. We start with 3000 simulations. \
Data is being generated in R using neuRosim (for a look at the code, I again refer to the IBMAvsMA report).


## Second level GLM
In this section we can compare the estimates of our effect size (g) with COPE values at the second level GLM. This is mere a check point as the COPE values are linearly related to the effect size! We will only look at the **first study** of the data (which contains 5 studies). Next, we can compare the variance of our estimated effect size (which is calculated using the formula of Radua) with the VARCOPE at the second level. These are not linearly related to each other!


```{r echo=FALSE}
# Reset working directory
rm(list=ls())
# Set starting seed
set.seed(11121990)
# WD
wd <- "/Users/hanbossier/Dropbox/PhD/PhDWork/Meta Analysis/R Code/Studie_Simulation/SimulationGit"
# Directories of the data for different takes
DATAwd <- list(
  'Take[MAvsIBMA]' = "/Volumes/2_TB_WD_Elements_10B8_Han/PhD/Simulation/Results/MAvsIBMA"
	)
NUMDATAwd <- length(DATAwd)
currentWD <- 1
# Number of conficence intervals
CIs <- c('MA-weightVar','GLM-t')
NumCI <- length(CIs)
# Number of executed simulations
nsim.tmp <- matrix(c(
                1,3000
              ), ncol=2, byrow=TRUE)
nsim <- nsim.tmp[currentWD,2]
# Number of subjects and studies
nsub <- 100
nstud <- 5
# Dimension of brain
DIM.tmp <- array(NA, dim=c(NUMDATAwd,3))
	DIM.tmp[c(1),] <- c(4,4,4)
DIM <- DIM.tmp[currentWD,]
# True value
trueVal <- 0
# Load in libraries
library(AnalyzeFMRI)
library(fmri)
library(lattice)
library(gridExtra)
library(oro.nifti)
library(ggplot2)
library(reshape2)
library(RColorBrewer)
library(Hmisc)
library(devtools)
library(neuRosim)
library(scatterplot3d)
# Function for data wrangling: indicator for CI and true value
indicating <- function(UPPER, LOWER, trueVal){
	IND <- trueVal >= LOWER & trueVal <= UPPER
	COVERAGE <- apply(IND, 1, mean, na.rm=TRUE)
	return(COVERAGE)
}
# Function for levelplot
ValuesOnLevelPlot2D <- function(x, y, z, ...) {
    panel.levelplot(x,y,z,...)
    panel.text(x, y, round(z,3),col='red')
}
# Load in functions from FixRan study
source('~/Dropbox/PhD/PhDWork/Meta\ Analysis/R\ Code/Studie_FixRan/FixRanStudyGit.git/Development/functions.R')
```

```{r echo=FALSE, cache=TRUE}
#### First load in all the data
AllData <- c()
# Load in the data
for(i in 1:nsim){
  load(paste(DATAwd[[currentWD]],'/',i,'/SCEN_1/ObjectsMAvsIBMA_',i,sep=''))
  AllData <- c(AllData, matrix(unlist(ObjectsMAvsIBMA), ncol=1))
}
# Make nsim number of columns
AllData <- matrix(AllData,ncol=nsim)
# Load the naming structure of the data
load(paste(DATAwd[[currentWD]],'/1/SCEN_1/ObjectsMAvsIBMA_1',sep='')); objects <- names(ObjectsMAvsIBMA); rm(ObjectsMAvsIBMA)
OBJ.ID <- c(rep(objects[!objects %in% c("STHEDGE","STWEIGHTS")], each=prod(DIM)), rep(c("STHEDGE","STWEIGHTS"), each=c(prod(DIM)*nstud)))
#########################################################
###################### CI COVERAGE ######################
#########################################################
# Calculate coverage
mean.coverage.weightVar.MA <-
  mean.coverage.t.IBMA <-
  array(NA,dim=c(prod(DIM),1))
mean.coverage.weightVar.MA[,1] <- indicating(UPPER = AllData[which(OBJ.ID=='CI.MA.upper.weightVar'),],LOWER = AllData[which(OBJ.ID=='CI.MA.lower.weightVar'),],trueVal = trueVal)
mean.coverage.t.IBMA[,1] <- indicating(UPPER = AllData[which(OBJ.ID=='CI.IBMA.upper.t'),],LOWER = AllData[which(OBJ.ID=='CI.IBMA.lower.t'),],trueVal = trueVal)
# Put the 2 coverages in a list
mean.coverages <- list('MA' = mean.coverage.weightVar.MA,'IBMA' = mean.coverage.t.IBMA)
# Mean over all voxels
CI.coverages <- data.frame(
	'Mean' = matrix(sapply(mean.coverages, FUN=function(...){apply(...,2,mean)}),ncol=1),
	'SD' = matrix(sapply(mean.coverages, FUN=function(...){apply(...,2,sd)}),ncol=1),
	'CI' = factor(CIs, levels=CIs, labels=CIs)
	)
#########################################################
####################### CI LENGTH #######################
#########################################################
# Calculate CI length
mean.length.weightVar.MA <-
mean.length.t.IBMA <-
array(NA,dim=c(prod(DIM),1))
mean.length.weightVar.MA[,1] <- apply(AllData[which(OBJ.ID=='CI.MA.upper.weightVar'),] - AllData[which(OBJ.ID=='CI.MA.lower.weightVar'),],1,mean)
mean.length.t.IBMA[,1] <- apply(AllData[which(OBJ.ID=='CI.IBMA.upper.t'),] - AllData[which(OBJ.ID=='CI.IBMA.lower.t'),],1,mean)
# Put the 2 lengths in a list
mean.lengths <- list('MA' = mean.length.weightVar.MA,'IBMA' = mean.length.t.IBMA)
# Average over all voxels
CI.lengths <- data.frame(
	'Mean' = matrix(sapply(mean.lengths, FUN=function(...){apply(...,2,mean)}),ncol=1),
	'SD' = matrix(sapply(mean.lengths, FUN=function(...){apply(...,2,sd)}),ncol=1),
	'CI' = factor(CIs, levels=CIs, labels=CIs)
	)
#########################################################
################### STANDARDIZED BIAS ###################
#########################################################
MA.SDBETA <- apply(AllData[which(OBJ.ID=='MA.WeightedAvg'),],1,sd)
MA.MEANBETA <- apply(AllData[which(OBJ.ID=='MA.WeightedAvg'),],1,mean)
IBMA.SDBETA <- apply(AllData[which(OBJ.ID=='IBMA.COPE'),],1,sd)
IBMA.MEANBETA <- apply(AllData[which(OBJ.ID=='IBMA.COPE'),],1,mean)
mean.bias.MA <- matrix(((abs(MA.MEANBETA)-trueVal)/(MA.SDBETA))*100,ncol=1)
mean.bias.IBMA <- matrix(((abs(IBMA.MEANBETA)-trueVal)/(IBMA.SDBETA))*100,ncol=1)
# Put the 2 bias values in a list
mean.bias <- list('MA' = mean.bias.MA,'IBMA' = mean.bias.IBMA)
# Average over all voxels
CI.bias <- data.frame(
	'Mean' = matrix(sapply(mean.bias, FUN=function(...){apply(...,2,mean)}),ncol=1),
	'SD' = matrix(sapply(mean.bias, FUN=function(...){apply(...,2,sd)}),ncol=1),
	'CI' = factor(CIs, levels=CIs, labels=c('MA', 'GLM'))
	)
# I want to compare COPE maps with hedges' g on second level. Then the varcope and variance of hedges's
# Start with COPE vs Hedges'g of the first study
copeStud <- hedgeStud <- array(NA, dim=c(prod(DIM),nsim))
varcopeStud <- stweights <- array(NA, dim=c(prod(DIM),nsim))
for(i in 1:nsim){
  copeStud[,i] <- readNIfTI(paste(DATAwd[[currentWD]],'/',i,'/SCEN_1/study1_stats/cope1.nii', sep=''), verbose=FALSE, warn=-1, reorient=TRUE, call=NULL)[,,]
    hedgeAllStud <- AllData[which(OBJ.ID=='STHEDGE'),i]
  hedgeStud[,i] <- hedgeAllStud[c(1:prod(DIM))]

  varcopeStud[,i] <- readNIfTI(paste(DATAwd[[currentWD]],'/',i,'/SCEN_1/study1_stats/varcope1.nii', sep=''), verbose=FALSE, warn=-1, reorient=TRUE, call=NULL)[,,]
    stweightsAllStud <- AllData[which(OBJ.ID=='STWEIGHTS'),i]
  stweights[,i] <- stweightsAllStud[c(1:prod(DIM))]
}
```

The first two QQ-plots show the distributions of the effect size and the COPE values. The first compares all values over all the 64 voxels and all simulations. The second is an average over all the 3000 simulations. Next we have a density with both distributions. 

```{r echo=FALSE, fig.align='center', fig.width=12, fig.height=7}
# Compare COPE with hedges g
  # Start with comparing over all values
  par(mfrow=c(1,2), oma=c(0,0,2,0))
  qqplot(x=copeStud, y= hedgeStud, xlab='Level 2 in GLM - all values', ylab='Meta-analysis approach')
  # Averaged over all simulations.
  qqplot(x=apply(copeStud,1,mean,na.rm=TRUE), y=apply(hedgeStud,1,mean,na.rm=TRUE),xlab='Level 2 in GLM - averaged', ylab='Meta-analysis approach')
  title("Q-Q plot comparing 2e level COPE and hedges g", outer=TRUE)

  # Histogram
  frameLvl2 <- data.frame('value' = matrix(c(hedgeStud,copeStud),ncol=1), 'source' = rep(c('g', 'COPE'), each = prod(dim(copeStud))))
  ggplot(frameLvl2, aes(x=value)) + geom_density(aes(colour=source),size=2) +
  theme(legend.position='top')
```

Now we will compare the variance of *g* with the VARCOPES at the second level. Again we start with a QQ-plot with all values, followed by a QQ-plot averaged over the simulations. Then we have a histogram. 

```{r, echo=FALSE, fig.align='center', fig.width=12, fig.height=7}
# Compare VARCOPE with variance of Weighted average
  # Over all values
  par(mfrow=c(1,2), oma=c(0,0,2,0))
  qqplot(x=varcopeStud, y= c(1/stweights), xlab='Level 2 in GLM', ylab='Meta-analysis approach')
  # Averaged over all simulations
  qqplot(x=apply(varcopeStud,1,mean,na.rm=TRUE), y=c(1/apply(stweights, 1, mean, na.rm=TRUE)),
    xlab='Level 2 in GLM', ylab = 'Meta-analysis approach')
  title("Q-Q plot comparing 2e level VARCOPE and variance of g in the first study", outer=TRUE)
  par(mfrow=c(1,1))
  # Histogram
  frameLvl2 <- data.frame('value' = matrix(c(as.numeric(1/stweights),as.numeric(varcopeStud)),ncol=1), 'source' = rep(c('variance g', 'VARCOPE'), each = prod(dim(copeStud))))
  ggplot(frameLvl2, aes(x=value)) + geom_histogram(aes(fill=source)) +
  theme(legend.position='top')
```

```{r}
summary(as.numeric(1/stweights))
summary(as.numeric(varcopeStud))
```

We can see how the variance values differ from each other. The distributions do not generally stack well against each other. The variance of our estimated effect size is 1) lower than the VARCOPE values and 2) more evenly distributed (not very visible on the figure) within its range of values.

## Third level GLM
Next, we can look at the third level in the analysis chain. This corresponds to the weighted average in the classical meta-analysis compared to the COPE values at level 3. And then the weighted variance of this estimated weighted average is compared against the VARCOPES at level 3.

We will start again with two QQ-plots (one with all values and one averaged over all simulations) comparing the weighted averages with the COPE maps. We have 3000 times a grid of 64 values (simulations x voxels).


```{r, echo=FALSE, cache=TRUE}
# We will go to all the cope and varcopes of each study
copeMA <- array(NA, dim=c(prod(DIM),nsim))
varcopeMA <- array(NA, dim=c(prod(DIM),nsim))
for(i in 1:nsim){
  copeMA[,i] <- readNIfTI(paste(DATAwd[[currentWD]],'/',i,'/SCEN_1/MA_stats/cope1.nii', sep=''), verbose=FALSE, warn=-1, reorient=TRUE, call=NULL)[,,]
  varcopeMA[,i] <- readNIfTI(paste(DATAwd[[currentWD]],'/',i,'/SCEN_1/MA_stats/varcope1.nii', sep=''), verbose=FALSE, warn=-1, reorient=TRUE, call=NULL)[,,]
}
```

```{r, echo=FALSE, fig.align='center', fig.width=12, fig.height=7}
# Compare COPE with Weighted average
  # Start with comparing over all values
  par(mfrow=c(1,2), oma=c(0,0,2,0))
  qqplot(x=copeMA, y= AllData[which(OBJ.ID=='MA.WeightedAvg'),], xlab='Level 3 in GLM - all values', ylab='Meta-analysis approach')
  # Averaged over all simulations (which we already did in previous part).
  qqplot(x=apply(copeMA,1,mean,na.rm=TRUE), y=MA.MEANBETA,xlab='Level 3 in GLM - averaged over simulations', ylab = 'Meta-analysis approach')
  title("Q-Q plot comparing 3e level COPE and weighted average in the MA", outer=TRUE)

  # Histogram
  frameLvl3 <- data.frame('value' = matrix(c(AllData[which(OBJ.ID=='MA.WeightedAvg'),],copeMA),ncol=1), 'source' = rep(c('WAvg', 'COPE3'), each = prod(dim(copeMA))))
  ggplot(frameLvl3, aes(x=value)) + geom_density(aes(colour=source),size=2) +
  theme(legend.position='top')
```

Same for the weighted variance of the estimated weighted average, compared to the VARCOPE maps at level 3.

```{r, echo=FALSE, fig.align='center', fig.width=12, fig.height=7}
par(mfrow=c(1,2), oma=c(0,0,2,0))
# Compare VARCOPE with variance of Weighted average
  # Over all values
  qqplot(x=varcopeMA, y= AllData[which(OBJ.ID=='CI.MA.weightedVariance'),], xlab='Level 3 in GLM - all values', ylab='Meta-analysis approach')
  # Averaged over all simulations
  qqplot(x=apply(varcopeMA,1,mean,na.rm=TRUE), y=apply(AllData[which(OBJ.ID=='CI.MA.weightedVariance'),], 1, mean, na.rm=TRUE),
    xlab='Level 3 in GLM - averaged over simulations', ylab = 'Meta-analysis approach')
  title("Q-Q plot comparing 3e level VARCOPE and weighted variance in MA", outer=TRUE)

  # Histogram
  frameLvl3 <- data.frame('value' = matrix(c(AllData[which(OBJ.ID=='CI.MA.weightedVariance'),],varcopeMA),ncol=1), 'source' = rep(c('WVar', 'VARCOPE3'), each = prod(dim(copeMA))))
  ggplot(frameLvl3, aes(x=value)) + geom_density(aes(colour=source),size=2) +
  theme(legend.position='top')
```
  
```{r}
# Summary of the weighted variance of the estimated weighted average
summary(as.numeric(AllData[which(OBJ.ID=='CI.MA.weightedVariance'),]))
# Summary of the VARCOPE maps
summary(as.numeric(varcopeMA))
```




## COPE and VARCOPE ==> Classical Meta-Analysis. 
During some of the meetings, the question came up what would happen if we send the COPE and VARCOPE maps at stage 2 to the meta-analysis procedure at stage 3. Hence we get for the weighted average ($\hat{\mu}$):

$$ \hat{\mu} = \frac{\sum_i\hat{w_i}\hat{COPE}_i}{\sum_i \hat{w_i}}$$ 
With $\hat{w_i}$: 

$$ \hat{w_i} = \frac{1}{\hat{VARCOPE}} $$

We could theoretically construct a CI around this new weighted average by first weighting the VARCOPE images through its inverse ($\hat{w_i}$): 
$$ \hat{V_w}(\hat{\mu}) = \frac{\sum_i\hat{w_i}(\hat{COPE_i} - \hat{\mu})^2}{(k - 1)\sum_i\hat{w_i}} $$

and then use this estimated variance in:

$$ \hat{\mu} \pm t_{k-1,1-\alpha/2}\sqrt{\hat{V_w}(\hat{\mu})} $$


Hence it is possible to calculate the standardized bias of this new weighted average, the length of the CI and the coverage around the true value = 0 (as in the IBMAvsMA report). \
I will define the meta-analysis based on the COPE and VARCOPE maps as *MACV* (meta-analysis based on COPE-VARCOPE) in the figures.

```{r echo=FALSE, cache=TRUE}
# We can calculate a weighted average of the COPE values using the VARCOPES from the second level.
# Furthermore we can calculate the CI using the weighted variance, also based on the COPE and VARCOPE.
weightedAverage <- upper.weightVar <- lower.weightVar <- array(NA, dim=c(prod(DIM),nsim))
# We will go to all the cope and varcopes of each study
for(i in 1:nsim){
  copeStud <- array(NA, dim=c(prod(DIM),nstud))
  varcopeStud <- array(NA, dim=c(prod(DIM),nstud))
  for(j in 1:nstud){
    copeStud[,j] <- readNIfTI(paste(DATAwd[[currentWD]],'/',i,'/SCEN_1/study',j,'_stats/cope1.nii', sep=''), verbose=FALSE, warn=-1, reorient=TRUE, call=NULL)[,,]
    varcopeStud[,j] <- readNIfTI(paste(DATAwd[[currentWD]],'/',i,'/SCEN_1/study',j,'_stats/varcope1.nii', sep=''), verbose=FALSE, warn=-1, reorient=TRUE, call=NULL)[,,]
  }
  # First caluclate the weighted average
  weightedAverage[,i] <- rowSums(copeStud*(1/varcopeStud))/rowSums((1/varcopeStud))
  # Now the weighted variance and CI
  weightVar <- (apply(((1/varcopeStud)*(copeStud - weightedAverage[,i])^2),c(1),sum))/((nstud - 1) * apply((1/varcopeStud),1,sum))
  upper.weightVar[,i] <- matrix(weightedAverage[,i],ncol=1) + (qt(0.975,df=nstud-1) * sqrt(matrix(weightVar,ncol=1)))
  lower.weightVar[,i] <- matrix(weightedAverage[,i],ncol=1) - (qt(0.975,df=nstud-1) * sqrt(matrix(weightVar,ncol=1)))
}
# Average the weighted average over the simulations
MACV.MEANBETA <- apply(weightedAverage,1,mean,na.rm=TRUE)
```

After calculating the *MACV* values, we can already look at both the distributions of MACV and the weighted averages through a classic meta-analysis (using *g* and its variance).
```{r echo=FALSE, fig.align='center', fig.width=12, fig.height=7}
MACV.MEANBETA <- apply(weightedAverage,1,mean,na.rm=TRUE)
# Stack both distributions next to each other
colours <- c('#238b45','#2171b5')
MEANBETAS <- data.frame('Value' = c(MA.MEANBETA, MACV.MEANBETA), 'source' = rep(c('MA', 'COPE-VARCOPE'), each=prod(DIM)))
  ggplot(MEANBETAS, aes(x=Value, group=source)) + geom_density(aes(colour=source),size=2) +
  scale_x_continuous(name="Weighted average") +
  scale_colour_manual(values = colours, name='Source', labels = c('COPE-VARCOPE -> MA', "g-variance -> MA")) + 
    theme(legend.position='top')
```


The standardized bias of the 3 analysis chains (MACV, MA and GLM) are depicted next. First we show the averages over all simulations from the 64 voxels. Then averaged over the voxels and simulations. 

```{r echo=FALSE}
#### ------ Standardized bias ------ ####
MACV.SDBETA <- apply(weightedAverage,1,sd)
mean.bias.MACV <- matrix(((abs(MACV.MEANBETA)-trueVal)/(MACV.SDBETA))*100,ncol=1)
# Let us compare this bias with the previously calculated ones
MACV.mean.bias <- list('MACV' = mean.bias.MACV,'MA' = mean.bias.MA, 'GLM' = mean.bias.IBMA)
# Average over all voxels
CVCI.bias <- data.frame(
	'Mean' = matrix(sapply(MACV.mean.bias, FUN=function(...){apply(...,2,mean)}),ncol=1),
	'SD' = matrix(sapply(MACV.mean.bias, FUN=function(...){apply(...,2,sd)}),ncol=1),
	'CI' = factor(c('COPE-VARCOPE->MA',CIs), levels=c('COPE-VARCOPE->MA',CIs), labels=c('MACV','MA', 'GLM'))
	)
```

```{r echo=FALSE, fig.align='center', fig.width=14, fig.height=7}
BCI1 <- levelplot(array(mean.bias[['MA']], dim=rep(sqrt(prod(DIM)),2)),
      col.regions = gray(100:0/100), at=seq(0,ceiling(max(unlist(mean.bias))),length.out=100), main='Meta-Analysis',xlab='',ylab='',
        colorkey = list(space = "bottom"),
               panel=ValuesOnLevelPlot2D)
BCI2 <- levelplot(array(mean.bias[['IBMA']], dim=rep(sqrt(prod(DIM)),2)),
      col.regions = gray(100:0/100), at=seq(0,ceiling(max(unlist(mean.bias))),length.out=100), main='3 level GLM',xlab='',ylab='',
              colorkey = list(space = "bottom"),
              panel=ValuesOnLevelPlot2D)
BMACV <- levelplot(array(MACV.mean.bias[['MACV']], dim=rep(sqrt(prod(DIM)),2)),
      col.regions = gray(100:0/100), at=seq(0,ceiling(max(unlist(MACV.mean.bias))),length.out=100), main='COPE-VARCOPE -> MA',xlab='',ylab='',
              colorkey = list(space = "bottom"),
              panel=ValuesOnLevelPlot2D)
grid.arrange(BCI1,BCI2,BMACV,nrow=1, top = textGrob('Standardized bias (%) of each voxel over 3000 simulations.', gp=gpar(fontsize=20,font=1)))
CVCI.bias
```

The same can be done for the lenght of the CIs.


```{r echo=FALSE, fig.align='center', fig.width=14, fig.height=7}
#### ------ CI length ------ ####
mean.length.MACV <- array(NA,dim=c(prod(DIM),1))
mean.length.MACV[,1] <- apply(upper.weightVar - lower.weightVar,1,mean)
  # Compare this with previously calculated CI lengths
  MACV.mean.length <- list('MACV' = mean.length.MACV, 'MA' = mean.lengths$MA, 'GLM' = mean.lengths$IBMA)
  # Average over all voxels
  CVCI.length <- data.frame(
    'Mean' = matrix(sapply(MACV.mean.length, FUN=function(...){apply(...,2,mean)}),ncol=1),
    'SD' = matrix(sapply(MACV.mean.length, FUN=function(...){apply(...,2,sd)}),ncol=1),
    'CI' = factor(c('COPE-VARCOPE->MA',CIs), levels=c('COPE-VARCOPE->MA',CIs), labels=c('MACV','MA', 'GLM'))
    )
LCI1 <- levelplot(array(mean.lengths[['MA']], dim=rep(sqrt(prod(DIM)),2)),
      col.regions = gray(100:0/100), at=seq(0,1,by=0.05), main='Meta-Analysis',xlab='',ylab='',
        colorkey = list(space = "bottom"),
               panel=ValuesOnLevelPlot2D)
LCI2 <- levelplot(array(mean.lengths[['IBMA']], dim=rep(sqrt(prod(DIM)),2)),
      col.regions = gray(100:0/100), at=seq(0,1,by=0.05), main='3 level GLM',xlab='',ylab='',
              colorkey = list(space = "bottom"),
              panel=ValuesOnLevelPlot2D)
LEMACV <- levelplot(array(MACV.mean.length[['MACV']], dim=rep(sqrt(prod(DIM)),2)),
      col.regions = gray(100:0/100), at=seq(0,1,by=0.05), main='COPE-VARCOPE -> MA',xlab='',ylab='',
              colorkey = list(space = "bottom"),
              panel=ValuesOnLevelPlot2D)
grid.arrange(LCI1,LCI2,LEMACV,nrow=1, top = textGrob('Average CI length of each voxel over 3000 simulations.', gp=gpar(fontsize=20,font=1)))
CVCI.length
```

Finally, we look at the average coverage as well. 

```{r echo=FALSE, fig.align='center', fig.width=14, fig.height=7}
#### ------ CI coverage ------ ####
mean.coverage.MACV <- array(NA,dim=c(prod(DIM),1))
mean.coverage.MACV[,1] <- indicating(UPPER = upper.weightVar,LOWER = lower.weightVar,trueVal = trueVal)
  # Compare this with previously calculated CI coverages.
  MACV.mean.CI <- list('MACV' = mean.coverage.MACV,'MA' = mean.coverages$MA, 'GLM' = mean.coverages$IBMA)
  # Average over all voxels
  CVCI.bias <- data.frame(
  	'Mean' = matrix(sapply(MACV.mean.CI, FUN=function(...){apply(...,2,mean)}),ncol=1),
  	'SD' = matrix(sapply(MACV.mean.CI, FUN=function(...){apply(...,2,sd)}),ncol=1),
  	'CI' = factor(c('COPE-VARCOPE->MA',CIs), levels=c('COPE-VARCOPE->MA',CIs), labels=c('MACV','MA', 'GLM'))
  	)
CCI1 <- levelplot(array(mean.coverages[['MA']], dim=rep(sqrt(prod(DIM)),2)),
      col.regions = gray(0:100/100), at=seq(0.75,1,by=0.05), main='Meta-Analysis',xlab='',ylab='',
        colorkey = list(space = "bottom"),
               panel=ValuesOnLevelPlot2D)
CCI2 <- levelplot(array(mean.coverages[['IBMA']], dim=rep(sqrt(prod(DIM)),2)),
      col.regions = gray(0:100/100), at=seq(0.75,1,by=0.05), main='3 level GLM',xlab='',ylab='',
              colorkey = list(space = "bottom"),
              panel=ValuesOnLevelPlot2D)
CIMACV <- levelplot(array(MACV.mean.CI[['MACV']], dim=rep(sqrt(prod(DIM)),2)),
      col.regions = gray(0:100/100), at=seq(0.75,1,by=0.05), main='COPE-VARCOPE -> MA',xlab='',ylab='',
              colorkey = list(space = "bottom"),
              panel=ValuesOnLevelPlot2D)
grid.arrange(CCI1,CCI2,CIMACV,nrow=1, top = textGrob('Average coverage of each voxel over 3000 simulations.', gp=gpar(fontsize=20,font=1)))
CVCI.bias
```




