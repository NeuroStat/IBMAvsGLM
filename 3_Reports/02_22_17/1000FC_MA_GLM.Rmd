---
title: "IBMA vs GLM"
author: "Han Bossier"
date: "22 februari 2017"
output:
  html_document:
    toc: yes
  html_notebook:
    theme: united
    toc: yes
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	comment = NA
)
```


## Introduction

In this report, we look at two approaches to do third level pooling of fMRI data. Either we use a meta-analysis approach. Or we apply a third level GLM on top of the two-stage GLM procedure. 
This report is based on the _1000FC resting state_ part of the project (next to simulations).

First, we have some theory, then the procedure and (preliminary) results.

## Theory

### Meta-analysis
The first approach originates from the meta-analysis literature (mostly for clinical trials). The idea is to calculate a weighted average of the effect sizes of all studies obtained through literature search. Then, we can construct a confidence interval (CI) around this weighted average. Details are given below:

#### Mathematical definition
Let us have $k$ studies with $\hat{\theta_i}$ the estimated effect size in study $i$. Consider for a meta-analysis the estimator for a weighted average ($\mu$) as:

$$ \hat{\mu} = \frac{\sum_i\hat{w_i}\hat{\theta}_i}{\sum_i \hat{w_i}}$$ 
with $\hat{w_i}$ the estimated optimal weights (either fixed or random effects meta-analysis). \
The sampling variance of $\hat{\mu}$ is given by: 
$$ V(\hat{\mu}) =\frac{1}{\sum_i\hat{w_i}}$$

In the report *coverages*, we have investigated the optimal method to construct such a confidence interval. This is based on a study by [Sanchez-Meca and Martin-Martinez, 2008](https://www.ncbi.nlm.nih.gov/pubmed/18331152).
This is based on weighting the observed variance of $\hat{\mu}$ before constructing a t-based CI. \
The weighted variance CI assumes a Student's *t* distribution with $k-1$ degrees of freedom. The sampling variance of $\hat{\mu}$ is extended by applying the following weighting scheme:
$$ \hat{V_w}(\hat{\mu}) = \frac{\sum_i\hat{w_i}(\hat{\theta_i} - \hat{\mu})^2}{(k - 1)\sum_i\hat{w_i}} $$

The CI around the weighted average effect size can then be calculated as:
$$ \hat{\mu} \pm t_{k-1,1-\alpha/2}\sqrt{\hat{V_w}(\hat{\mu})} $$

Note that by choosing the optimal weighting structure $\hat{w_i}$, we can either ignore between study variability (by choosing a fixed effects meta-analysis). Or incorporate this (random effects meta-analysis).

### GLM approach.
The GLM approach extends the traditional way to analyze an fMRI experiment. Typically, we will use a two-stage GLM when analyzing fMRI data. The first stage, within subjects, relates the design of the experiment to the observed BOLD signal in each voxel in the brain. The second stage pools subjects according to a group design (between subjects). In this stage, we can assume that the true signal varries between subjects (which is a mixed model). \
At this stage, we can add an extra stage to the GLM procedure by re-using the mixed effects model at stage 2 with individual studies as input. Effectively, we get stage 1-2-3 leading to *within* subject - *between* subject - *between sudies*.

#### Mathematical definition
##### Level 1
At each stage, we calculate the COPE and VARCOPES which are being sent to the next level in the GLM. \
The 3 level GLM starts at the first level by relating the design of the experiment to the observed time series for each individual. For a given voxel and each subject *k*, we have:

$$ Y_k = X_k\beta_k+\epsilon_k  $$

With $Y_k$ the $T \times 1$ observed BOLD time series at that voxel, $X_k$ the $T \times p$ design matrix (containing *p* regressors), $\beta_k$ a $p \times 1$ vector of parameters and finally $\epsilon_k$ the $T \times 1$ error term. 
The first level COPE map is estimated by:

$$ c\hat{\beta}_k=c(X'_kX_k)^{-1}X'_kY_k $$

Here we assume assume no autocorrelation in the time series. The VARCOPE map is calculated by:

$$ \hat{Var}(c\hat{\beta}_k) = (X'_kX_k)^{-1}c'\hat{\sigma}^2_k $$
With:
$$ \hat{\sigma}^2_k = (Y - X\hat{\beta}_k)'(Y - X\hat{\beta}_k)/(T - p) $$

##### Level 2
The second level GLM, we apply a group model:

$$ Y_G = X_G\beta_G+\epsilon^{*}_G $$
With $X_G$ the $N \times p_G$ group-level design matrix, $\beta_G$ the group level parameter vector and $\epsilon^{*}_G$ the imperfect estimated group error (due to using first level contrasts) vector in which $Var(\epsilon_G)$ contains both intrasubject as well as between-subject variance. The group level parameters are estimated through:

$$ \hat{\beta_G} = X^{-}_GY_G $$ In which $^{-}$ denotes the pseudo-inverse. \
The GLS approach which is used in FSL's FLAME1 mixed effects approach assumes that:
$$ Var(\epsilon^{*}_G) = \sigma^2I_N + Var_{\beta}(Y_G) $$ 

In which the $\beta$ subscript denotes this being intrasubject variance. The estimates of these variances are used from the first level. The $\sigma^2_G$ is estimated through an iterative estimation algorithm (pressumable REML).

##### Level 3
Finally, the third level repeats the second level with the object of interest now being studies instead of subjects. Hence, we apply a study model:

$$ Y_S = X_S\beta_S+\epsilon^{**}_S $$

In the end, we would like to calculate a confidence interval around the contrast of parameter estimates (the beta coefficients belonging to the contrast of interest). The GLS approach in FSL estimates the parameters by assuming a multivariate non-central t-distribution with $k-1$ degrees of freedom. Hence, we calculate the CI as:

$$ \hat{\beta}_S \pm t_{k-1,1-\alpha/2}\sqrt{\hat{Var}(\hat{\beta}_S)} $$


## Performance Measures
We will check the performance of the two methods by modeling task fMRI designs onto resting state fMRI (1000FC project) after which we calculate the:

  1. standardized bias
  2. average confidence interval length
  3. coverage.
  
Consider the estimate of interest $\hat{\theta}$ either being the weighted average in the meta-analysis or the $\beta_S$ parameter in the third level GLM. Then the standardized bias is defined as:

$$ \left(\frac{\bar{\hat{\theta}} - \theta}{SE(\hat{\theta})})\right) \times 100 $$

The average confidence interval length is obtained by subtracting the upper limit of the CI from the lower limit and dividing by $B$, the number of iterations.

The coverage is the proportion of times the $100 (1 - \alpha)\%$ CI contains the true value $\theta = 0$.



## 1000FC

More details can be found at this [Github page](https://github.com/NeuroStat/IBMAvsGLM/blob/c0d310a04bc22511d74f595d0b00aede9a3e34c1/1_Scripts/CI_IBMAvsGLM/1000FC/README.md).


### Third Level
At this moment, we have 2500 iterations in which we sample 20 subjects at level 2 into 5 studies at level 3. No subjects are sampled twice in one iteration! 




## Analysis
#### Parameters
First we load in functions and set some parameters:
```{r}
# Date of today
date <- Sys.Date()
print(paste('Running code at ', date, sep = ''))

# Set starting seed
set.seed(11121990)

# Define WD
wd <- "/Users/hanbossier/Dropbox/PhD/PhDWork/Meta Analysis/R Code/Studie_Simulation/SimulationGit/2_Analyses/CI_GLMvsIBMA/1000FC"

# Directories of the data for different scenario's
DATAwd <- list(
  'Take[8mmBox10]' = "/Volumes/2_TB_WD_Elements_10B8_Han/PhD/IBMAvsGLM/Results/Cambridge/ThirdLevel/8mm/boxcar10"
	)
NUMDATAwd <- length(DATAwd)
currentWD <- 1

# Number of conficence intervals
CIs <- c('MA-weightVar','GLM-t')
NumCI <- length(CIs)

# Number of executed runs
nruns.tmp <- matrix(c(
                1,2500
              ), ncol=2, byrow=TRUE)
nruns <- nruns.tmp[currentWD,2]

# Number of subjects and studies
nsub <- 20
nstud <- 5

# Dimension of brain
DIM <- c(91,109,91)

# True value
trueVal <- 0

# Load in libraries
library(oro.nifti)
library(dplyr)
library(lattice)
library(grDevices)
library(ggplot2)

# Function to count the number of instances in which true value is between lower and upper CI.
indicator <- function(UPPER, LOWER, trueval){
  IND <- trueval >= LOWER & trueval <= UPPER
  IND[is.na(IND)] <- 0
  return(IND)
}

# Funtion to count the number of recorded values
counting <- function(UPPER, LOWER){
  count <- (!is.na(UPPER) & !is.na(LOWER))
  return(count)
}
```


#### Mask
For each third level, we have a mask (0-1 map) which is the intersection of all copes coming from the second level. To remove edge effects when comparing over all iterations, we first calculate an universal mask over all iterations. This is again the intersection (`apply(AllMask, 1, prod)`) of all masks coming from the third levels. 

```{r}
# Set warnings off
options(warn = -1)

# Vector to check progress
CheckProgr <- floor(seq(1,nruns,length.out=10))

# Vector of simulations where we have a missing mask
missingMask <- c()

# Do you want to make an universal mask again?
WRITEMASK <- FALSE
if(isTRUE(WRITEMASK)){
  # Vector with all masks in it
  AllMask <- c()

  # Load in the masks
  for(i in 1:nruns){
    # Print progress
    if(i %in% CheckProgr) print(paste('LOADING MASKS. NOW AT ', (i/nruns)*100, '%', sep = ''))

    # Try reading in mask, then go to one column and convert to data frame.
    CheckMask <- try(readNIfTI(paste(DATAwd[[currentWD]], '/', i,'/mask.nii', sep = ''))[,,,1] %>%
              matrix(.,ncol = 1) %>% data.frame(), silent = TRUE)
      # If there is no mask, skip iteration
      if(class(CheckMask) == "try-error"){ missingMask <- c(missingMask, i); next}

      # Some masks are broken: if all values are zero: REPORT
      if(all(CheckMask == 0)){print(paste("CHECK MASK AT ITERATION ", i, sep = "")); next}

    # Bind the masks of all iterations together
    AllMask <- bind_cols(AllMask, CheckMask)
    rm(CheckMask)
  }

  # Take product to have universal mask
  UnivMask <- apply(AllMask, 1, prod)

  # Better write this to folder
  niftiimage <- nifti(img=array(UnivMask, dim = DIM),dim=DIM)
  writeNIfTI(niftiimage,filename=paste(DATAwd[[currentWD]],'/universalMask',sep=''),gzipped=FALSE)
}
if(isTRUE(!WRITEMASK)){
  # Read in mask
  UnivMask <- readNIfTI(paste(DATAwd[[currentWD]],'/universalMask.nii', sep = ''))[,,] %>%
            matrix(.,ncol = 1)
}
```

The amount of masked voxels:
```{r}
# Amount of masked voxels
sum(UnivMask)
```

#### CI coverage
All objects coming from third level (copes, varcopes, weighted averages, CIs from both procedures, hedges'g and weights of MA) are contained in one *R* object per iteration. We first load in the names and define the vectors containing CI coverage.
```{r}
# Load the naming structure of the data
load(paste(paste(DATAwd[[currentWD]], '/1/ObjectsRestMAvsGLM_1.RData',sep=''))); objects <- names(ObjectsRestMAvsGLM); rm(ObjectsRestMAvsGLM)
OBJ.ID <- c(rep(objects[!objects %in% c("STHEDGE","STWEIGHTS")], each=prod(DIM)), rep(c("STHEDGE","STWEIGHTS"), each=c(prod(DIM)*nstud)))

objects.CI <- objects[grepl(c('upper'), objects) | grepl(c('lower'), objects)]

# Pre-define the CI coverage, length and bias vectors in which we sum the values
# After running nruns, divide by amount of obtained runs.
summed.coverage.IBMA <-
  summed.coverage.GLM <-
  array(0,dim=c(sum(UnivMask == 1),1))

# Keeping count of amount of values
counterMA <- counterGLM <- 0
```


Now we loop over all iterations (or as much iterations as possible, now 100) and count whether true value is in CI for both procedures YES/NO.
```{r cache = TRUE}
# Load in the data
t1 <- Sys.time()
for(i in 1:1000){
  if(i %in% CheckProgr) print(paste('PROCESSING. NOW AT ', (i/nruns)*100, '%', sep = ''))
  CheckObj <- try(load(paste(DATAwd[[currentWD]], '/', i, '/ObjectsRestMAvsGLM_', i, '.RData', sep = '')), silent = TRUE)
    if(class(CheckObj) == "try-error"){print(paste('Missing data in iteration ', i, sep = '')); next}

    # CI coverage: loop over the two procedures
    for(p in 1:2){
      objUP <- objects.CI[grepl(c('upper'), objects.CI)][p]
      objLOW <- objects.CI[grepl(c('lower'), objects.CI)][p]
      UP <- ObjectsRestMAvsGLM[[objUP]] %>% data.frame() %>% filter(., UnivMask == 1)
      LOW <- ObjectsRestMAvsGLM[[objLOW]] %>% data.frame() %>% filter(., UnivMask == 1)
      if(grepl('MA', x = objUP)){
        summed.coverage.IBMA[,1] <- summed.coverage.IBMA[,1] +
          indicator(UPPER = UP, LOWER = LOW, trueval = 0)
        counterMA <- counterMA + counting(UPPER = UP, LOWER = LOW)
      }else{
        summed.coverage.GLM[,1] <- summed.coverage.GLM[,1] +
          indicator(UPPER = UP, LOWER = LOW, trueval = 0)
        counterGLM <- counterGLM + counting(UPPER = UP, LOWER = LOW)
      }
      rm(objUP, objLOW, UP, LOW)
    }
    rm(CheckObj)
}

Sys.time() - t1
```

To calculate the coverage, we divide the coverage by the amount of times there was an upper and lower limit CI (counter[NAME]]). Note that we use a rather cumbersome way to calculate the CI coverage. Namely, we loop over iterations and add an indicator value (true value in CI, yes/no = 1/0) to an object. And later on divide this with the amount of times we have recoreded CIs.
This is because we quickly run into memory problems (unable to load in all simulations at once). Hence I am in the process of finding the most efficient way to calculate this.

The vectors *summed.coverage.[NAME]* are the sum of indicator values (true value in CI: 1 or 0) in each masked voxel. 

To recap: 

* *counterMA* or *counterGLM* are two vectors of length `sum(UnivMask == 1)`. They contain indicator values whether or not (1/0) we have an upper limit AND a lower limit of the CI.
* *summed.coverage.MA* and *summed.coverage.GLM* are vectors with the summed indicator values (true value in CI: 1 or 0) and length `sum(UnivMask == 1)`.

Let us check some descriptive statistics of these last two vectors:

```{r}
summary(summed.coverage.IBMA)
summary(summed.coverage.GLM)
```

How many values have we recorded?

```{r}
sample_n(data.frame(counterMA), size = 10)
table(counterMA)
sample_n(data.frame(counterGLM), size = 10)
table(counterGLM)
```

We can already see that in some iterations for the GLM procedure, there are less values recorded in some voxels. Note that the MA approach is based on the same cope map as the one used in the GLM procedure! 

Can we locate these voxels?
```{r fig.width = 10, fig.height = 12}
emptBrain <- array(0, dim = prod(DIM))
emptBrain[which(UnivMask == 1)] <- counterGLM
  # Put non-masked values to NA
  emptBrain[which(UnivMask == 0)] <- NA
  # Remove values where we have 95% of the data
  MinValue <- table(counterGLM)[table(counterGLM) > quantile(table(counterGLM), .95)] %>% 
              names() %>% min() %>% as.numeric()
  emptBrain[which(emptBrain >= MinValue)] <- 0

# Now plot
levelplot(array(emptBrain, dim = DIM), col.regions = terrain.colors)

# Zoom in at slice z = 70
levelplot(array(emptBrain, dim = DIM)[,,70], col.regions = terrain.colors)
```

So, it seems to involve edge effects. Note that this means we have less data at those voxels. This does not imply that the coverage is low at these edges.

We will now turn to the distribution of the coverages. We start with a histogram of the observed coverages in all masked voxels. 
```{r}
# Histogram
hist(summed.coverage.IBMA/counterMA, main = 'Coverages in all masked voxels - IBMA')
hist(summed.coverage.GLM/counterGLM, main = 'Coverages in all masked voxels - GLM')
```

Or through a violin plot of the coverages over all voxels:
```{r}
# Violin plot
ViolinPlot <- data.frame(Coverages = c((summed.coverage.IBMA/counterMA),
                                    (summed.coverage.GLM/counterGLM)),
                        Method = rep(c('IBMA', 'GLM'), each = length(summed.coverage.IBMA)))
ggplot(ViolinPlot, aes(x = Method, y=Coverages)) + geom_violin() + coord_flip() +
  ggtitle("Violin plot of coverages in all voxels")
```

Summary and averages of these coverages over all voxels:
```{r}
# Summary of coverages and average over all voxels in MA
summary(summed.coverage.IBMA/counterMA)
mean(summed.coverage.IBMA/counterMA)


# Summary of coverages and average over all voxels in GLM
summary(summed.coverage.GLM/counterGLM)
mean(summed.coverage.GLM/counterGLM)

```


Now we observe voxels which have a low coverage, _consistently_ over all simulations in the GLM procedure. We can check whether these are the same voxels as the ones observed above.



```{r fig.width = 10, fig.height = 12}
# Create empty brain
emptBrain <- array(0, dim = prod(DIM))

# Add the summed coverage values of GLM
emptBrain[which(UnivMask == 1)] <- summed.coverage.GLM
  # Put non-masked values to NA
  emptBrain[which(UnivMask == 0)] <- NA
  # Already try levelplot 
  levelplot(array(emptBrain, dim = DIM), col.regions = terrain.colors)
  # Zoom in at slice z = 70
  levelplot(array(emptBrain, dim = DIM)[,,70], col.regions = terrain.colors)
  # Remove values where we have 95% of the data
  MinValue <- table(summed.coverage.GLM)[table(summed.coverage.GLM) > quantile(table(summed.coverage.GLM), .95)] %>% 
              names() %>% min() %>% as.numeric()
  emptBrain[which(emptBrain >= MinValue)] <- -10

unique(emptBrain)    

# Now plot with the top 95% voxels being set to -10
levelplot(array(emptBrain, dim = DIM), col.regions = terrain.colors)

# Now plot with the top 95% voxels being set to -10 and remove them from colour scale
levelplot(array(emptBrain, dim = DIM), col.regions = c('#ffffff', terrain.colors(n = length(unique(emptBrain)) - 2 )))

# Zoom in at slice z = 70
levelplot(array(emptBrain, dim = DIM)[,,70], col.regions = c('#ffffff', terrain.colors(n = length(unique(emptBrain)) - 2 )))
```



Look at voxel with lowest coverage.
Where is it?

```{r}
# We found out voxel number 48554 of the UnivMask == 1 vector has low coverage.
# Where is this voxel located in 3D space?
rowID <- seq(1,length(UnivMask)) %>% data.frame(rowID = .)
voxID <- UnivMask %>% data.frame(UnivMask = .) %>% bind_cols(.,rowID) %>%
  filter(., UnivMask == 1) %>% slice(48554)

# On top of a mask
MaskVoxel <- UnivMask
MaskVoxel[voxID$rowID] <- 2
levelplot(array(MaskVoxel, dim = DIM), col.regions = rainbow(n = 3),cuts = 2)
# zoom in
levelplot(array(MaskVoxel, dim = DIM)[,,30], col.regions = rainbow(n = 3),cuts = 2)
```

T-test:

```{r}
# Read in 20 random simulations and gather info on this voxel.
voxCOPE <- c()

for(i in sample(x = 1:2500, size = 20)){
  tmp <- try(readNIfTI(paste(DATAwd[[currentWD]], '/', i, '/STCOPE.nii', sep = ''))[,,,1], silent = TRUE)
  if(class(tmp) == 'try-error') next
  voxCOPE <- c(voxCOPE, array(tmp, dim = prod(DIM))[voxID$rowID])
}

voxCOPE
t.test(voxCOPE, alternative = 'two.sided')

```


We discovered voxels that are significant over 20 iterations. Remeber that we contrast blocks of condition 1 (i.e. "*activation*") with rest (condition 2). The negative *t*-value indicates that there are voxels who are more associated with these rest blocks. We should try another design!

> If all designs (either block or event related designs) are contrasts between "activation" and rest, then shouldn't we always expect significant (negative) results?

Note (!): I did not yet look whether we observe positive voxels that are significant over 20 iterations. 











